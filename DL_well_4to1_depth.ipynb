{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4028054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored 'well/log4tutorial.xlsx'\n",
      "Checked out revision 52.\n"
     ]
    }
   ],
   "source": [
    "## A tutorial script for well log prediction \n",
    "# This tutorial is for the most general case:\n",
    "# inputing 4 values from four existing well logs and predicting the missing S-wave log (DT)\n",
    "# The advance in this script compared with the earlier one is the usage of depth constraint\n",
    "# \n",
    "# INPUT: P-wave sonic log (DTCO), Gamma-ray (ECGR), Density (RHOB), Total porosity (PHIT)\n",
    "# OUTPUT: S-wave sonic log (DTSM)\n",
    "\n",
    "# first download the data from\n",
    "# Using SVN (deprecated since 01/08/2024)\n",
    "# !svn co https://github.com/chenyk1990/mldata/trunk/well ./well \n",
    "# well4tutorial2.xlsx\n",
    "\n",
    "# Using Git\n",
    "# git clone -n --depth=1 --filter=tree:0 https://github.com/chenyk1990/mldata\n",
    "# cd mldata/\n",
    "# git sparse-checkout set --no-cone well\n",
    "# git checkout\n",
    "# cp well/well4tutorial2.xlsx ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcf703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 14:11:36.749978: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)    [(None, 8, 4)]               0         []                            \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 8, 16)                832       ['input_layer[0][0]']         \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 16)                   0         ['bidirectional[0][0]']       \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 16)                0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1, 2)                 32        ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1, 16)                32        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 8, 16)                0         ['bidirectional[0][0]',       \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 8, 16)                0         ['multiply[0][0]',            \n",
      "                                                                     'bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 8, 32)                4224      ['add[0][0]']                 \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 32)                   0         ['bidirectional_1[0][0]']     \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 32)                0         ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1, 4)                 128       ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1, 32)                128       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)       (None, 8, 32)                0         ['bidirectional_1[0][0]',     \n",
      "                                                                     'dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 8, 32)                0         ['multiply_1[0][0]',          \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 8, 64)                16640     ['add_1[0][0]']               \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 64)                   0         ['bidirectional_2[0][0]']     \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 1, 64)                0         ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1, 8)                 512       ['reshape_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1, 64)                512       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 8, 64)                0         ['bidirectional_2[0][0]',     \n",
      "                                                                     'dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 8, 64)                0         ['multiply_2[0][0]',          \n",
      "                                                                     'bidirectional_2[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirecti  (None, 128)                  66048     ['add_2[0][0]']               \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 8)                    1032      ['bidirectional_3[0][0]']     \n",
      "                                                                                                  \n",
      " output_layer (Activation)   (None, 8)                    0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90120 (352.03 KB)\n",
      "Trainable params: 90120 (352.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here we introduce a self-attention BiLSTM network for training a model\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten, Reshape, multiply\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "def SE_Block(tensor, ratio=8):\n",
    "    init = tensor\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = int(init.shape[-1])\n",
    "    se_shape = (1, filters)\n",
    "    se = GlobalAveragePooling1D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    x = multiply([init, se])\n",
    "    x = Add()([x,init])\n",
    "    return x\n",
    "\n",
    "inp1 = layers.Input(shape=(8,4),name='input_layer')\n",
    "D1 = 8\n",
    "D2 = int(2*D1)\n",
    "D3 = int(2*D2)\n",
    "D4 = int(2*D3)\n",
    "D5 = int(2*D4)\n",
    "\n",
    "features = Bidirectional(LSTM(D1, return_sequences=True))(inp1)\n",
    "features = SE_Block(features)\n",
    "features = Bidirectional(LSTM(D2, return_sequences=True))(features)\n",
    "features = SE_Block(features)\n",
    "features = Bidirectional(LSTM(D3, return_sequences=True))(features)\n",
    "features = SE_Block(features)\n",
    "features = Bidirectional(LSTM(D4, return_sequences=False))(features)\n",
    "\n",
    "#features = Dense(D4,activation='relu')(features)\n",
    "#features = Dense(D3,activation='relu')(features)\n",
    "#features = Dense(D2,activation='relu')(features)\n",
    "\n",
    "e = Dense(8)(features)\n",
    "o = Activation('linear', name='output_layer')(e)\n",
    "model = Model(inputs=[inp1], outputs=o)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36848944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 18782.8867 - mse: 18782.8867\n",
      "Epoch 1: val_loss improved from inf to 14220.02539, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 29s 67ms/step - loss: 18775.3047 - mse: 18775.3047 - val_loss: 14220.0254 - val_mse: 14220.0254 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "  1/274 [..............................] - ETA: 15s - loss: 15399.7607 - mse: 15399.7607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenyk/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/274 [============================>.] - ETA: 0s - loss: 11036.3594 - mse: 11036.3594\n",
      "Epoch 2: val_loss improved from 14220.02539 to 8333.79785, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 11033.5938 - mse: 11033.5938 - val_loss: 8333.7979 - val_mse: 8333.7979 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 6398.5371 - mse: 6398.5371\n",
      "Epoch 3: val_loss improved from 8333.79785 to 4796.23584, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 20s 75ms/step - loss: 6396.5264 - mse: 6396.5264 - val_loss: 4796.2358 - val_mse: 4796.2358 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 3707.4165 - mse: 3707.4165\n",
      "Epoch 4: val_loss improved from 4796.23584 to 2839.70312, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 21s 77ms/step - loss: 3706.6689 - mse: 3706.6689 - val_loss: 2839.7031 - val_mse: 2839.7031 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 2294.4333 - mse: 2294.4333\n",
      "Epoch 5: val_loss improved from 2839.70312 to 1877.63562, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 21s 75ms/step - loss: 2292.9355 - mse: 2292.9355 - val_loss: 1877.6356 - val_mse: 1877.6356 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 1640.8770 - mse: 1640.8770\n",
      "Epoch 6: val_loss improved from 1877.63562 to 1469.77344, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1640.8770 - mse: 1640.8770 - val_loss: 1469.7734 - val_mse: 1469.7734 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1385.0006 - mse: 1385.0006\n",
      "Epoch 7: val_loss improved from 1469.77344 to 1324.70959, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 20s 74ms/step - loss: 1384.3212 - mse: 1384.3212 - val_loss: 1324.7096 - val_mse: 1324.7096 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1300.9619 - mse: 1300.9619\n",
      "Epoch 8: val_loss improved from 1324.70959 to 1282.22192, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 1300.5521 - mse: 1300.5521 - val_loss: 1282.2219 - val_mse: 1282.2219 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1278.6410 - mse: 1278.6410\n",
      "Epoch 9: val_loss improved from 1282.22192 to 1272.36401, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 88ms/step - loss: 1278.5728 - mse: 1278.5728 - val_loss: 1272.3640 - val_mse: 1272.3640 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1273.6865 - mse: 1273.6865\n",
      "Epoch 10: val_loss improved from 1272.36401 to 1270.44958, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 20s 75ms/step - loss: 1274.0978 - mse: 1274.0978 - val_loss: 1270.4496 - val_mse: 1270.4496 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1273.6713 - mse: 1273.6713\n",
      "Epoch 11: val_loss improved from 1270.44958 to 1270.11353, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 88ms/step - loss: 1273.3065 - mse: 1273.3065 - val_loss: 1270.1135 - val_mse: 1270.1135 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1272.4917 - mse: 1272.4917\n",
      "Epoch 12: val_loss improved from 1270.11353 to 1270.09302, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 20s 73ms/step - loss: 1273.2386 - mse: 1273.2386 - val_loss: 1270.0930 - val_mse: 1270.0930 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1274.3573 - mse: 1274.3573\n",
      "Epoch 13: val_loss did not improve from 1270.09302\n",
      "274/274 [==============================] - 20s 75ms/step - loss: 1273.2151 - mse: 1273.2151 - val_loss: 1270.1320 - val_mse: 1270.1320 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1272.4723 - mse: 1272.4723\n",
      "Epoch 14: val_loss did not improve from 1270.09302\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 1273.2465 - mse: 1273.2465 - val_loss: 1270.1285 - val_mse: 1270.1285 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1273.4318 - mse: 1273.4318\n",
      "Epoch 15: val_loss improved from 1270.09302 to 1270.03674, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 1273.2324 - mse: 1273.2324 - val_loss: 1270.0367 - val_mse: 1270.0367 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 1273.2087 - mse: 1273.2087\n",
      "Epoch 16: val_loss did not improve from 1270.03674\n",
      "274/274 [==============================] - 24s 87ms/step - loss: 1273.2087 - mse: 1273.2087 - val_loss: 1270.0663 - val_mse: 1270.0663 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1272.1699 - mse: 1272.1699\n",
      "Epoch 17: val_loss did not improve from 1270.03674\n",
      "274/274 [==============================] - 21s 78ms/step - loss: 1273.2816 - mse: 1273.2816 - val_loss: 1270.0381 - val_mse: 1270.0381 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1272.0626 - mse: 1272.0626\n",
      "Epoch 18: val_loss did not improve from 1270.03674\n",
      "274/274 [==============================] - 22s 79ms/step - loss: 1273.2267 - mse: 1273.2267 - val_loss: 1270.1978 - val_mse: 1270.1978 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1272.3871 - mse: 1272.3871\n",
      "Epoch 19: val_loss did not improve from 1270.03674\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 1273.2633 - mse: 1273.2633 - val_loss: 1270.1547 - val_mse: 1270.1547 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 1200.9081 - mse: 1200.9081\n",
      "Epoch 20: val_loss improved from 1270.03674 to 1002.52515, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 22s 80ms/step - loss: 1199.7903 - mse: 1199.7903 - val_loss: 1002.5251 - val_mse: 1002.5251 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 829.5639 - mse: 829.5639\n",
      "Epoch 21: val_loss improved from 1002.52515 to 697.43506, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 27s 98ms/step - loss: 829.0702 - mse: 829.0702 - val_loss: 697.4351 - val_mse: 697.4351 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 661.2642 - mse: 661.2642\n",
      "Epoch 22: val_loss improved from 697.43506 to 605.01886, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 661.2642 - mse: 661.2642 - val_loss: 605.0189 - val_mse: 605.0189 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 590.0835 - mse: 590.0835\n",
      "Epoch 23: val_loss improved from 605.01886 to 565.45355, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 88ms/step - loss: 590.1471 - mse: 590.1471 - val_loss: 565.4536 - val_mse: 565.4536 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 546.4404 - mse: 546.4404\n",
      "Epoch 24: val_loss improved from 565.45355 to 518.26868, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 22s 79ms/step - loss: 546.0567 - mse: 546.0567 - val_loss: 518.2687 - val_mse: 518.2687 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 517.5417 - mse: 517.5417\n",
      "Epoch 25: val_loss improved from 518.26868 to 498.32065, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 22s 79ms/step - loss: 517.5417 - mse: 517.5417 - val_loss: 498.3206 - val_mse: 498.3206 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 488.3350 - mse: 488.3350\n",
      "Epoch 26: val_loss did not improve from 498.32065\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 489.3407 - mse: 489.3407 - val_loss: 506.3369 - val_mse: 506.3369 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 473.4240 - mse: 473.4240\n",
      "Epoch 27: val_loss improved from 498.32065 to 472.49081, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 89ms/step - loss: 473.0594 - mse: 473.0594 - val_loss: 472.4908 - val_mse: 472.4908 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 463.4780 - mse: 463.4780\n",
      "Epoch 28: val_loss did not improve from 472.49081\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 463.7161 - mse: 463.7161 - val_loss: 504.3482 - val_mse: 504.3482 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 447.0055 - mse: 447.0055\n",
      "Epoch 29: val_loss improved from 472.49081 to 436.68427, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 21s 76ms/step - loss: 446.6671 - mse: 446.6671 - val_loss: 436.6843 - val_mse: 436.6843 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 438.5668 - mse: 438.5668\n",
      "Epoch 30: val_loss did not improve from 436.68427\n",
      "274/274 [==============================] - 24s 89ms/step - loss: 438.7494 - mse: 438.7494 - val_loss: 448.3935 - val_mse: 448.3935 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 427.2463 - mse: 427.2463\n",
      "Epoch 31: val_loss improved from 436.68427 to 413.21335, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 86ms/step - loss: 427.3055 - mse: 427.3055 - val_loss: 413.2133 - val_mse: 413.2133 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 423.1454 - mse: 423.1454\n",
      "Epoch 32: val_loss did not improve from 413.21335\n",
      "274/274 [==============================] - 23s 83ms/step - loss: 423.0967 - mse: 423.0967 - val_loss: 430.6010 - val_mse: 430.6010 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 422.3835 - mse: 422.3835\n",
      "Epoch 33: val_loss did not improve from 413.21335\n",
      "274/274 [==============================] - 23s 84ms/step - loss: 422.3835 - mse: 422.3835 - val_loss: 422.5197 - val_mse: 422.5197 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 408.5113 - mse: 408.5113\n",
      "Epoch 34: val_loss improved from 413.21335 to 412.19907, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 23s 82ms/step - loss: 408.7849 - mse: 408.7849 - val_loss: 412.1991 - val_mse: 412.1991 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 405.3044 - mse: 405.3044\n",
      "Epoch 35: val_loss improved from 412.19907 to 398.64783, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 405.3382 - mse: 405.3382 - val_loss: 398.6478 - val_mse: 398.6478 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 400.4441 - mse: 400.4441\n",
      "Epoch 36: val_loss did not improve from 398.64783\n",
      "274/274 [==============================] - 22s 81ms/step - loss: 400.4297 - mse: 400.4297 - val_loss: 413.1149 - val_mse: 413.1149 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 398.6563 - mse: 398.6563\n",
      "Epoch 37: val_loss did not improve from 398.64783\n",
      "274/274 [==============================] - 24s 88ms/step - loss: 398.2333 - mse: 398.2333 - val_loss: 417.7912 - val_mse: 417.7912 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 395.5225 - mse: 395.5225\n",
      "Epoch 38: val_loss did not improve from 398.64783\n",
      "274/274 [==============================] - 25s 91ms/step - loss: 395.2724 - mse: 395.2724 - val_loss: 408.0456 - val_mse: 408.0456 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 395.8250 - mse: 395.8250\n",
      "Epoch 39: val_loss improved from 398.64783 to 389.70883, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 21s 78ms/step - loss: 396.1467 - mse: 396.1467 - val_loss: 389.7088 - val_mse: 389.7088 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 387.3712 - mse: 387.3712\n",
      "Epoch 40: val_loss improved from 389.70883 to 382.46857, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 24s 88ms/step - loss: 387.3712 - mse: 387.3712 - val_loss: 382.4686 - val_mse: 382.4686 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 384.3035 - mse: 384.3035\n",
      "Epoch 41: val_loss did not improve from 382.46857\n",
      "274/274 [==============================] - 24s 86ms/step - loss: 384.3035 - mse: 384.3035 - val_loss: 404.9228 - val_mse: 404.9228 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 384.7036 - mse: 384.7036\n",
      "Epoch 42: val_loss did not improve from 382.46857\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 384.5176 - mse: 384.5176 - val_loss: 385.3639 - val_mse: 385.3639 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 378.7680 - mse: 378.7680\n",
      "Epoch 43: val_loss improved from 382.46857 to 375.10254, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 20s 75ms/step - loss: 378.7459 - mse: 378.7459 - val_loss: 375.1025 - val_mse: 375.1025 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 378.0508 - mse: 378.0508\n",
      "Epoch 44: val_loss did not improve from 375.10254\n",
      "274/274 [==============================] - 22s 79ms/step - loss: 377.8181 - mse: 377.8181 - val_loss: 404.5891 - val_mse: 404.5891 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 374.0749 - mse: 374.0749\n",
      "Epoch 45: val_loss improved from 375.10254 to 373.27603, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 21s 76ms/step - loss: 373.8577 - mse: 373.8577 - val_loss: 373.2760 - val_mse: 373.2760 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 371.2446 - mse: 371.2446\n",
      "Epoch 46: val_loss did not improve from 373.27603\n",
      "274/274 [==============================] - 21s 76ms/step - loss: 371.4303 - mse: 371.4303 - val_loss: 380.6435 - val_mse: 380.6435 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 364.0526 - mse: 364.0526\n",
      "Epoch 47: val_loss improved from 373.27603 to 363.51111, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 25s 93ms/step - loss: 363.7640 - mse: 363.7640 - val_loss: 363.5111 - val_mse: 363.5111 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 370.7409 - mse: 370.7409\n",
      "Epoch 48: val_loss did not improve from 363.51111\n",
      "274/274 [==============================] - 20s 74ms/step - loss: 370.7409 - mse: 370.7409 - val_loss: 372.8581 - val_mse: 372.8581 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - ETA: 0s - loss: 373.4336 - mse: 373.4336\n",
      "Epoch 49: val_loss improved from 363.51111 to 358.19986, saving model to best_model_well4tutorial2.h5\n",
      "274/274 [==============================] - 22s 79ms/step - loss: 373.4336 - mse: 373.4336 - val_loss: 358.1999 - val_mse: 358.1999 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "273/274 [============================>.] - ETA: 0s - loss: 362.1738 - mse: 362.1738\n",
      "Epoch 50: val_loss did not improve from 358.19986\n",
      "274/274 [==============================] - 23s 85ms/step - loss: 361.9590 - mse: 361.9590 - val_loss: 372.8703 - val_mse: 372.8703 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Bebig the training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow.keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras,os\n",
    "sgd = tensorflow.keras.optimizers.Adam(lr=1e-3)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                                cooldown= 0,\n",
    "                                patience= 50,\n",
    "                                min_lr=0.5e-8)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(monitor= 'val_loss', patience = 100)\n",
    "\n",
    "filename='well4tutorial2.xlsx'\n",
    "for fname in [filename]:\n",
    "    df = pd.read_excel(fname)\n",
    "    df_raw_input = df[['DTCO', 'ECGR', 'RHOB', 'PHIT']]\n",
    "    X = df_raw_input[0:] #df_raw_input[0:-4] in case it is not dividable\n",
    "    y = df['DTSM'][0:]   #df['DTSM'][0:-4] in case it is not dividable\n",
    "    \n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    X=np.reshape(X,[8,50000,4],order='F').transpose(1,0,2);#.reshape(107777,32,order='F')\n",
    "    y=np.reshape(y,[8,50000],order='F').transpose(1,0)\n",
    "    plt.plot(y[0,:]);plt.show()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "# \tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0)\n",
    "    checkpoint = ModelCheckpoint(filepath='best_model_%s.h5'%(fname.split('.')[0].split('/')[-1]),\n",
    "                             monitor='val_loss',\n",
    "                             mode = 'auto',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "    callbacks = [lr_reducer, early_stopping_monitor, checkpoint]\n",
    "    model = Model(inputs=[inp1], outputs=o)\n",
    "    model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\n",
    "    history = model.fit([X_train], y_train, epochs=50, validation_data=([X_test],y_test), shuffle=True, batch_size=128, callbacks = callbacks)\n",
    "#100 or 200 epochs will definitely better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97098290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 4s 4ms/step\n",
      "well4tutorial2 R2: 0.717947823992266  RMSE: 18.925946383275843\n"
     ]
    }
   ],
   "source": [
    "## Below is to test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import os\n",
    "filename='well4tutorial2.xlsx'\n",
    "for fname in [filename]:\n",
    "    df = pd.read_excel(fname)\n",
    "\n",
    "    df_raw_input = df[['DTCO', 'ECGR', 'RHOB', 'PHIT']]\n",
    "    X = df_raw_input[0:]\n",
    "    y = df['DTSM'][0:]\n",
    "    \n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    X=np.reshape(X,[8,50000,4],order='F').transpose(1,0,2);#.reshape(50000,32,order='F')\n",
    "    y=np.reshape(y,[8,50000],order='F').transpose(1,0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "# \tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0)\n",
    "    model = load_model('best_model_%s.h5'%(fname.split('.')[0].split('/')[-1])) #current one:  0.8361\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(fname.split('.')[0].split('/')[-1]+' R2:', r2,' RMSE:',rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3170669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenyk/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:508: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return column_or_1d(y, warn=True)\n",
      "/Users/chenyk/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well4tutorial2 R2: 0.6269690696049068  RMSE: 21.766044103176505\n",
      "469/469 [==============================] - 3s 3ms/step\n",
      "well4tutorial2 R2: 0.7179596366649222  RMSE: 18.926169089683395\n"
     ]
    }
   ],
   "source": [
    "## Comparison with random forest\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import os\n",
    "filename='well4tutorial2.xlsx'\n",
    "for fname in [filename]:\n",
    "    df = pd.read_excel(fname)\n",
    "\n",
    "    df_raw_input = df[['DTCO', 'ECGR', 'RHOB', 'PHIT']]\n",
    "    X = df_raw_input[0:]\n",
    "    y = df['DTSM'][0:]\n",
    "    \n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    X=np.reshape(X,[8,50000,4],order='F').transpose(1,0,2);#.reshape(50000,32,order='F')\n",
    "    y=np.reshape(y,[8,50000],order='F').transpose(1,0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "bagged_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, max_samples=0.7, random_state=42)\n",
    "bagged_model.fit(X_train.reshape(35000*8,4,order='F'), y_train.reshape(35000*8,1,order='F'))\n",
    "\n",
    "y_pred = bagged_model.predict(X_test.reshape(15000*8,4,order='F'))\n",
    "r2 = r2_score(y_test.reshape(15000*8,1,order='F'), y_pred)\n",
    "rmse = mean_squared_error(y_test.reshape(15000*8,1,order='F'), y_pred, squared=False)\n",
    "print(fname.split('.')[0].split('/')[-1]+' R2:', r2,' RMSE:',rmse)\n",
    "\n",
    "# If BiLSTM has already been run; uncomment\n",
    "model = load_model('best_model_%s.h5'%(fname.split('.')[0])) #current one:  0.82\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
    "rmse = mean_squared_error(y_test.flatten(), y_pred.flatten(), squared=False)\n",
    "print(fname.split('.')[0].split('/')[-1]+' R2:', r2,' RMSE:',rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090720ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
